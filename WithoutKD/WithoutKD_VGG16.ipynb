{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WithoutKD_VGG16.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mshitie/MS_KD/blob/main/WithoutKD/WithoutKD_VGG16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install rarfile\n",
        "\n",
        "from google.colab import drive\n",
        "import rarfile\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to the .rar file in Google Drive\n",
        "rar_path = '/content/drive/MyDrive/computer_vision/main_data_croped.rar'\n",
        "\n",
        "# Destination folder to extract the contents\n",
        "destination_folder = '/content'\n",
        "\n",
        "# Create the destination folder if it doesn't exist\n",
        "if not os.path.exists(destination_folder):\n",
        "    os.makedirs(destination_folder)\n",
        "\n",
        "# Extract the .rar file\n",
        "with rarfile.RarFile(rar_path, 'r') as rar_ref:\n",
        "    rar_ref.extractall(destination_folder)\n",
        "\n",
        "print(\"Extraction completed.\")"
      ],
      "metadata": {
        "id": "lkjRX89XOEBd",
        "outputId": "d2daef95-b8ec-4280-ec62-9a35485a2a80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rarfile\n",
            "  Downloading rarfile-4.1-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: rarfile\n",
            "Successfully installed rarfile-4.1\n",
            "Mounted at /content/drive\n",
            "Extraction completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "import shutil\n",
        "\n",
        "def partition_images(directory, csv_file, output_folder):\n",
        "    image_files = []\n",
        "    folder_counts = {}  # Dictionary to store the count of images moved to each folder\n",
        "\n",
        "    # Read the CSV file\n",
        "    with open(csv_file, 'r') as file:\n",
        "        reader = csv.reader(file)\n",
        "        header = next(reader, None)  # Skip the header row if it exists\n",
        "\n",
        "        # Iterate over the rows in the CSV file\n",
        "        for row in reader:\n",
        "            image_file = row[0]  # Assuming the image file names are in the first column\n",
        "            label = row[1]  # Assuming the labels are in the second column\n",
        "            image_files.append((image_file, label))\n",
        "\n",
        "    # Create directories for each label\n",
        "    labels = set(label for _, label in image_files)\n",
        "    for label in labels:\n",
        "        label_dir = os.path.join(output_folder, label)\n",
        "        os.makedirs(label_dir, exist_ok=True)\n",
        "        folder_counts[label] = 0  # Initialize the count to 0\n",
        "\n",
        "    # Move the image files to separate label directories\n",
        "    for image_file, label in image_files:\n",
        "        source_path = os.path.join(directory, image_file + \".jpg\")  # Assuming the file extension is '.jpg'\n",
        "        destination_path = os.path.join(output_folder, label, image_file + \".jpg\")\n",
        "\n",
        "        if os.path.exists(source_path):\n",
        "            print(f\"Moving {source_path} to {destination_path}\")\n",
        "            shutil.move(source_path, destination_path)\n",
        "            folder_counts[label] += 1  # Increment the count for the corresponding folder\n",
        "\n",
        "        else:\n",
        "            print(f\"File not found: {source_path}\")\n",
        "\n",
        "    # Print the counts for each folder\n",
        "    print(\"\\nNumber of images moved in each folder:\")\n",
        "    for label, count in folder_counts.items():\n",
        "        print(f\"{label}: {count}\")\n",
        "\n",
        "# Directory path where the images and CSV file are located\n",
        "directory_path = '/content/main_data_croped/'\n",
        "csv_file_path = '/content/drive/MyDrive/computer_vision/tfti2.csv'\n",
        "output_folder = \"/content/main_data_croped2/\"\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Call the function to partition the images based on the labels\n",
        "partition_images(directory_path, csv_file_path, output_folder)"
      ],
      "metadata": {
        "id": "xT4OsX30OKEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import scipy\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn')\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import auc, roc_curve\n",
        "from tqdm import tqdm\n",
        "\n",
        "import time"
      ],
      "metadata": {
        "id": "YIGdygabOS4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keras & Tensorflow\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras import layers\n",
        "from keras.models import Model\n",
        "from keras.layers import Lambda, concatenate\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import GlobalAveragePooling2D , Conv2D , MaxPooling2D\n",
        "from keras.layers import  Dropout , BatchNormalization , Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from keras.callbacks import Callback , ReduceLROnPlateau , ModelCheckpoint\n",
        "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
        "from keras.losses import categorical_crossentropy as logloss\n",
        "from keras.metrics import categorical_accuracy"
      ],
      "metadata": {
        "id": "Ix2Ek3z1OVUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "train_dir = '/content/main_data_croped2'\n",
        "\n",
        "# Load train images\n",
        "tf_train = []\n",
        "for filename in os.listdir(os.path.join(train_dir, 'TF')):\n",
        "    img = Image.open(os.path.join(train_dir, 'TF', filename)).convert('RGB')\n",
        "    img = img.resize((224, 224))  # Resize if necessary\n",
        "    img_array = np.array(img)\n",
        "    tf_train.append(img_array)\n",
        "tf_train = np.array(tf_train)\n",
        "\n",
        "it_train = []\n",
        "for filename in os.listdir(os.path.join(train_dir, 'TI')):\n",
        "    img = Image.open(os.path.join(train_dir, 'TI', filename)).convert('RGB')\n",
        "    img = img.resize((224, 224))  # Resize if necessary\n",
        "    img_array = np.array(img)\n",
        "    it_train.append(img_array)\n",
        "it_train = np.array(it_train)\n",
        "\n",
        "normal_train = []\n",
        "for filename in os.listdir(os.path.join(train_dir, 'normal')):\n",
        "    img = Image.open(os.path.join(train_dir, 'normal', filename)).convert('RGB')\n",
        "    img = img.resize((224, 224))  # Resize if necessary\n",
        "    img_array = np.array(img)\n",
        "    normal_train.append(img_array)\n",
        "normal_train = np.array(normal_train)\n",
        "\n",
        "print('Done Loaded :)')\n",
        "\n",
        "# Shape of our dataset\n",
        "print(f'TF Train:', tf_train.shape)\n",
        "print(f'TI Train:', it_train.shape)\n",
        "print(f'Normal Train:', normal_train.shape)\n",
        "\n",
        "tf_train_label = np.zeros(len(tf_train), dtype=float)\n",
        "it_train_label = np.ones(len(it_train), dtype=float)\n",
        "normal_train_label = np.full(len(normal_train), 2, dtype=float)\n",
        "\n",
        "X_train = np.concatenate((tf_train, it_train, normal_train), axis=0)\n",
        "Y_train = np.concatenate((tf_train_label, it_train_label, normal_train_label), axis=0)\n",
        "\n",
        "s = np.arange(X_train.shape[0])\n",
        "np.random.shuffle(s)\n",
        "X_train = X_train[s]\n",
        "Y_train = Y_train[s]\n",
        "\n",
        "Y_train = to_categorical(Y_train, num_classes=3)\n",
        "\n",
        "print(f'X train shape:', X_train.shape)\n",
        "print(f'Y train shape:', Y_train.shape)\n",
        "\n",
        "# Split the data into training, validation, and testing sets\n",
        "x_train, x_temp, y_train, y_temp = train_test_split(X_train, Y_train, test_size=0.2, random_state=10)\n",
        "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=10)\n",
        "\n",
        "print(f'x train shape:', x_train.shape)\n",
        "print(f'x test shape:', x_test.shape)\n",
        "print(f'x val shape:', x_val.shape)\n",
        "print(f'y train shape:', y_train.shape)\n",
        "print(f'y test shape:', y_test.shape)\n",
        "print(f'y val shape:', y_val.shape)"
      ],
      "metadata": {
        "id": "Av4-w8GMOXxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow.keras.applications import VGG16\n",
        "\n",
        "def vgg16_model(backbone , lr = 1e-4):\n",
        "  modelvgg16 = Sequential()\n",
        "  modelvgg16.add(backbone)\n",
        "  modelvgg16.add(layers.GlobalAveragePooling2D())\n",
        "  modelvgg16.add(layers.Dropout(0.5))\n",
        "  modelvgg16.add(layers.BatchNormalization())\n",
        "  modelvgg16.add(layers.Dense(3 , activation='softmax'))\n",
        "\n",
        "  modelvgg16.compile(\n",
        "        loss='categorical_crossentropy',\n",
        "        optimizer=Adam(learning_rate=lr),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "  return modelvgg16\n",
        "\n",
        "\n",
        "vgg16 = VGG16(\n",
        "    weights = 'imagenet',\n",
        "    include_top = False,\n",
        "    input_shape = (224 , 224 , 3)\n",
        ")\n",
        "# call the model\n",
        "modelvgg16 = vgg16_model(vgg16 , lr = 1e-4)\n",
        "modelvgg16.build((None, 224, 224, 3))\n",
        "modelvgg16.summary()"
      ],
      "metadata": {
        "id": "XBACZH5POaZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the student model with MobileNet\n",
        "epochs = 20\n",
        "batch_size = 64\n",
        "student_his_vgg16 = modelvgg16.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_val, y_val))"
      ],
      "metadata": {
        "id": "72GK59pLOi0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_val_pred = modelvgg16.predict(x_val)\n",
        "print(f'The VGG16 Accuracy on the Validation Set:',accuracy_score(np.argmax(y_val, axis=1), np.argmax(Y_val_pred, axis=1)))\n",
        ""
      ],
      "metadata": {
        "id": "7Azhz3hPOmbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now let's check my Y_test values\n",
        "print(f'My Y_test values are:\\n' ,y_test)"
      ],
      "metadata": {
        "id": "oiKK1dmxOqLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now let's check my predcited values from X_test dataset\n",
        "import timeit\n",
        "\n",
        "start = timeit.default_timer()\n",
        "#Your statements here\n",
        "y_pred = modelvgg16.predict(x_test)\n",
        "print(f'My predicted Y_test values are:\\n' ,y_pred)\n",
        "\n",
        "stop = timeit.default_timer()\n",
        "print('\\nTime: ',stop - start,'sec')"
      ],
      "metadata": {
        "id": "kqk9s3N3OuLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now let's check the accuracy between the original & predicted (Y_test , y_pred)\n",
        "print(f'My accuracy on Student model with VGG16 on the Test set is:',accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1)))\n",
        ""
      ],
      "metadata": {
        "id": "Xl9u41gxO0BP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the classification report\n",
        "print(classification_report( np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1)))"
      ],
      "metadata": {
        "id": "a0ey58CsO4Ku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hZiSz6gQO7tm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}